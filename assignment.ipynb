{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c38049b1dfc9c208",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Natural Language Processing Assignment\n",
    "In this assignment, we will be working with a database of medical transcriptions. We will be experimenting with different data representations of these transcriptions which are discussed in the lecture video. Don't worryâ€”you won't have to write much code yourself. You will have to fill in some gaps in the code. Read the descriptions carefully so you understand what the code is doing so you can fill the gaps in properly.\n",
    "\n",
    "You'll want to run each cell (see the run button above) in order of the code. If you do not run them in order, errors might appear. Also, if there is something you need to fill in and you didn't, an error might appear as well. Read the instructions and run every block of code as you go along. Fill in gaps before running the code if instructed.\n",
    "\n",
    "With the introduction out of the way, let's start with the assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The Task\n",
    "\n",
    "The task we want to complete is that we want to extract keywords out of the transcripts. Then we'll create a wordcloud showing the most common and important words."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2445a2537bba152"
  },
  {
   "cell_type": "markdown",
   "id": "7a036ae99aeb40e6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Loading the data\n",
    "Firstly, we must let Python (the programming language we are using) load packages that can process our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "initial_id",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-06T17:59:33.925113600Z",
     "start_time": "2025-01-06T17:59:33.918598400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\PujaS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import nltk; nltk.download('stopwords'); from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "YOUR_CODE_HERE = None # Ignore this, this is so the file runs even if you do not fill in any code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be072877f700829",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Next we want to read the data. We get it from an online site called \"Huggingface\". If you want, you can view the information of the dataset [here](https://huggingface.co/datasets/tchebonenko/MedicalTranscriptions), although it is not necessary for the assignment. If you run the code block below, a table should appear, giving you a sense of what the data looks like. Note that some of the data will be cut off; Jupyter notebook is trying to show a sneak peek of the data, as the dataset is too big to show all of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff176760b12c44d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"hf://datasets/tchebonenko/MedicalTranscriptions/mtsamples.csv.zip\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0526ab54e0bf1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We want to analyse the transcription. In order to do so, we must clean the data first. We will make a new column where punctuation is removed, and the entire transcription is made to be lower case. We store this in a new column called `transcription_clean`.\n",
    "\n",
    "**Exercise 1.** Why do we want to clean up the data for this task? Do we always want to clean up the data in this manner for any NLP task? Why/why not?\n",
    "\n",
    "**Bonus.** If you answered we should always clean up the data, give some examples of other tasks where cleaning up data is important. If you answered we should not always clean up the data, give an example of a task where it would be counterproductive to do so. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c31f8f2ca418ea5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Enter your answer here\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "\n",
    "def clean_str(s):\n",
    "    new_s = str(s).lower()\n",
    "    new_s = new_s.translate(str.maketrans('', '', string.punctuation))\n",
    "    return new_s\n",
    "\n",
    "df['transcription_clean'] = df['transcription'].apply(lambda s: clean_str(s))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879cb98d0ce3523b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Furthermore, we want to turn the transcript into a list of words. We will store this in a new column called `tokens`. We do this as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53696ece5de1df8a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['tokens'] = df['transcription_clean'].apply(lambda s: s.split())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We want to count the frequency of any given word in the transcripts. We can do this by using the `Counter` function from the `collections` package, which will store the results in a dictionary. We will display the 10 most common words in the transcripts."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a05e0758986727f2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def count_words(token_column):\n",
    "    word_freq = Counter()\n",
    "\n",
    "    for i in range(len(df['tokens'])):\n",
    "        word_freq.update(token_column.iloc[i])\n",
    "    return word_freq\n",
    "\n",
    "word_freq = count_words(df['tokens'])\n",
    "word_freq.most_common(10)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "57b00ced53f3e4b0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "There's something off about the result. It does not seem to show the key-words we are after.\n",
    "**Exercise 2.** Can you tell what's wrong about this result? What do you think is the reason for this?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b985d43220227094"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Enter your answer here\n",
    "# \n",
    "# \n",
    "# \n",
    "# "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30430a3356061e05"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The set of common words in a language that do not necessarily convey meaning are called \"stopwords\". We want to remove these from the data. We can use the `stopwords` function from the `nltk.corpus` package to get a list of stopwords in English. We will then create a new column called `tokens_filtered` which will store the tokens without the stopwords."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "817c5f3215815548"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "df['tokens_filtered'] = df['tokens'].apply(lambda s: [word for word in s if word not in stop_words])\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b772fd5b826b34b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we will try to run the word frequency analysis again.\n",
    "**Exercise 3.** Fill in the code to run the word frequency analysis again. This time, display the 20 most common ones."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9507ab1033d1c6e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "word_freq = YOUR_CODE_HERE\n",
    "# YOUR CODE HERE (to display the 20 most common words)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "66eac052885e3dc0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Congratulations! You have finished the word frequency analysis. Now we will move on to creating a wordcloud. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b771422eb9f6343e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width=800, height=400, background_color ='white').generate_from_frequencies(word_freq)\n",
    "wordcloud.to_image()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cecfb0b2616a4b55"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
